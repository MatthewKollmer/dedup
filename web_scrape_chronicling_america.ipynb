{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Web Scraping Chronicling America OCR\n",
    "\n",
    "I crafted this program after needing to put together a dataset of newspaper-specific textual data from Chronicling America. I'm sharing it here in case others need to build similar datasets. To use this program, you'll need to identify the newspaper that has been digitized and OCR'ed in Chronicling America. You'll need at least one url for an OCR page of the given newspaper. It will be good to designate a timeframe of printing that you want to extract from the newspaper as well.\n",
    "\n",
    "This program utilizes patterns in Chronicling America's urls. For example, consider this page:\n",
    "\n",
    "https://chroniclingamerica.loc.gov/lccn/sn84024736/1814-09-28/ed-1/seq-1/ocr/\n",
    "\n",
    "You'll notice the url contains metadata. For example, 'sn84024736' is the Library of Congress's code for the given newspaper, the Inquirer, an early nineteenth century newspaper from Richmond, VA. Then there's the date: 1814-09-28. This tells you this url directs to the newspaper on that date. 'ed-1' refers to the edition scanned. 'seq-1' stands for sequence 1, or the first page of the newspaper on that day.\n",
    "\n",
    "With this information, I've been able to write this program which iterates over small changes to the url, extracts text from each combination of the url, and then removes any \"Not Found\" or dead links. The remaining output is a dataframe containing only the text for the given newspaper over the course of a predetermined period.\n",
    "\n",
    "Before running this program, please consider the following:\n",
    "\n",
    "- It can take a long time. Iterating over so many combinations of the url can take hours, or even days, depending on the range of dates you're web scraping. Given this limitation, you'll want to reduce the date range as much as possible. A shorter range equals fewer iterations, and subsequently less time and computer power needed.\n",
    "- Given the long timeframe this program could potentially take, be sure to deactivate sleep mode on your device. You need to be able to maintain VPN connection for this program to work.\n",
    "- This program is highly adaptable. The current version is aimed at extracting print periods for individual newspapers, but there are a number of ways of rewriting this code to fit other web scraping needs.\n",
    "- Double-check your url indices. Simply running this code without ensuring it is tailored to your specific url will be a waste of time and effort.\n",
    "- Further data preprocessing will probably be needed. This program only takes the html-formatted text from the urls and puts it into a dataframe. From there, you can munge the data in all sorts of ways. Just keep in mind this version of the program is limited to building a preliminary dataset, not a clean or processed one."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define your row-entry function, your dataframe, and your url.\n",
    "\n",
    "These preliminary steps will need to be double-checked within the rest of the program; that is, any changes to the demonstration versions below will also need to be changed throughout the program."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def insert_row(data, my_row):\n",
    "    data.loc[len(data)] = my_row\n",
    "\n",
    "df = pd.DataFrame(columns = ['url', 'text', 'date', 'not_found'])\n",
    "url_string = 'https://chroniclingamerica.loc.gov/lccn/sn84024736/1814-09-28/ed-1/seq-1/ocr/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Iteration Functions\n",
    "\n",
    "The following functions iterate over characters in url_string based on their indices. To identify the indices of a given character, you need to count the characters from left to right. So, the first character is 'h'. Its index is 0. The date indices in url_string are 51 to 60. You can double-check your indices with this line of code:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "'h'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_string[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "'1814-09-28'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_string[51:61]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Keep these indices in mind. To manipulate the program to extract different url pages, you'll need to adjust the following programs by changing their inputted indices."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def change_decade(string):\n",
    "    for i in range(0, 10): # Change range for preferred decade range. 0, 10 runs through all decades in the century.\n",
    "        # If you only want the first decade in the century, use range(0, 1). And so on.\n",
    "        new_character = str(i)\n",
    "        string_list = list(string)\n",
    "        index_to_change = 53\n",
    "        string_list[index_to_change] = new_character\n",
    "        modified_string = ''.join(string_list)\n",
    "        # print(modified_string) # add if you want to see the progress as the program runs\n",
    "        url = requests.get(modified_string)\n",
    "        text = url.text\n",
    "        insert_row(df, [modified_string, text, 'NaN', 'NaN'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def change_year(string):\n",
    "    for i in range(4, 6): # Change range for preferred year range. Do you only want years 1814-1815, for example?\n",
    "        # If so, range(4, 6) is correct. Do you want years 1810 to 1819? range(0,10) would be correct. And so on.\n",
    "        new_character = str(i)\n",
    "        string_list = list(string)\n",
    "        index_to_change = 54\n",
    "        string_list[index_to_change] = new_character\n",
    "        modified_string = ''.join(string_list)\n",
    "        # change_decade(modified_string) # Add this if you want to attempt scraping by decade.\n",
    "        # Note: this could take a very long time!\n",
    "        # print(modified_string) # add if you want to see the progress as the program runs\n",
    "        url = requests.get(modified_string)\n",
    "        text = url.text\n",
    "        insert_row(df, [modified_string, text, 'NaN', 'NaN'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# This function changes the first number in the month.\n",
    "def change_month(string):\n",
    "    for i in range(0, 2): # The first number in a month can only be 0 or 1 (i.e. july is 07, November is 11)\n",
    "        new_character = str(i)\n",
    "        string_list = list(string)\n",
    "        index_to_change = 56\n",
    "        string_list[index_to_change] = new_character\n",
    "        modified_string = ''.join(string_list)\n",
    "        change_year(modified_string) # Add this if you want to scrape by month.\n",
    "        # print(modified_string) # add if you want to see the progress as the program runs\n",
    "        url = requests.get(modified_string)\n",
    "        text = url.text\n",
    "        insert_row(df, [modified_string, text, 'NaN', 'NaN'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# This function changes the second number in the month.\n",
    "def change_month2(string):\n",
    "    for i in range(0, 10): # Second numbers in the month can be anything from 1 to 9.\n",
    "        new_character = str(i)\n",
    "        string_list = list(string)\n",
    "        index_to_change = 57\n",
    "        string_list[index_to_change] = new_character\n",
    "        modified_string = ''.join(string_list)\n",
    "        change_month(modified_string) # Add this if you want to scrape by month.\n",
    "        # print(modified_string) # add if you want to see the progress as the program runs\n",
    "        url = requests.get(modified_string)\n",
    "        text = url.text\n",
    "        insert_row(df, [modified_string, text, 'NaN', 'NaN'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# This function changes the first number in the day.\n",
    "def change_day(string):\n",
    "    for i in range(0, 4): # This range can only be 0 to 3, as in \"01 is the first day of the month, 31 is the last\".\n",
    "        new_character = str(i)\n",
    "        string_list = list(string)\n",
    "        index_to_change = 59\n",
    "        string_list[index_to_change] = new_character\n",
    "        modified_string = ''.join(string_list)\n",
    "        change_month2(modified_string) # Add this if you want to scrape by day.\n",
    "        # print(modified_string) # add if you want to see the progress as the program runs\n",
    "        url = requests.get(modified_string)\n",
    "        text = url.text\n",
    "        insert_row(df, [modified_string, text, 'NaN', 'NaN'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# This function changes the second number in the day.\n",
    "def change_day2(string):\n",
    "    for i in range(0, 10): # This range can be any number from 1 to 9.\n",
    "        new_character = str(i)\n",
    "        string_list = list(string)\n",
    "        index_to_change = 60\n",
    "        string_list[index_to_change] = new_character\n",
    "        modified_string = ''.join(string_list)\n",
    "        change_day(modified_string) # Add this if you want to scrape by day.\n",
    "        # print(modified_string) # add if you want to see the progress as the program runs\n",
    "        url = requests.get(modified_string)\n",
    "        text = url.text\n",
    "        insert_row(df, [modified_string, text, 'NaN', 'NaN'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def change_page(string):\n",
    "    for i in range(0, 5): # Change this range to fit the number of digitized pages per issue of given newspaper.\n",
    "        # You'll need to look this up on Chronicling America.\n",
    "        new_character = str(i)\n",
    "        string_list = list(string)\n",
    "        index_to_change = 71\n",
    "        string_list[index_to_change] = new_character\n",
    "        modified_string = ''.join(string_list)\n",
    "        change_day2(modified_string) # Add this if you want to scrape by the other functions.\n",
    "        # print(modified_string) # add if you want to see the progress as the program runs\n",
    "        url = requests.get(modified_string)\n",
    "        text = url.text\n",
    "        insert_row(df, [modified_string, text, 'NaN', 'NaN'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run the program\n",
    "\n",
    "Have you double-checked all the ranges, indices, and added or removed functions? If so, you can run the program:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# change_page(url_string)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now you wait. Eventually, you'll have a complete dataframe saved as df that contains all the urls and their text. But you're not done yet. df also contains text from \"Not Found\" urls or broken links. To filter this stuff out, try this:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [url, text, date, not_found]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>url</th>\n      <th>text</th>\n      <th>date</th>\n      <th>not_found</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['date'] = df['url'].str.findall(r'\\d\\d\\d\\d-\\d\\d-\\d\\d')\n",
    "df['not_found'] = df['text'].str.findall('dublincore.org')\n",
    "df['not_found'] = df['not_found'].astype(str)\n",
    "substring = 'dublincore.org'\n",
    "filter = df['not_found'].str.contains(substring)\n",
    "df = df[~filter]\n",
    "df = df.drop_duplicates(subset=['url'])\n",
    "df = df.sort_values('date')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This block of code fills the date column with the combination from the url. It finds strings of 'dublincore.org'–text that only appears on \"Not Found\" or broken link pages, and then filters those instances out of df. It also sorts df by date and drops any duplicates. The result should be a dataframe with only the text in html format from every Chronicling America url in your chosen ranges. Hooray!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [url, text, date, not_found]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>url</th>\n      <th>text</th>\n      <th>date</th>\n      <th>not_found</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Need to save your data as a csv file? Try changing the file name then running this line of code:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.to_csv('inquirer_1814-15.csv', index=False, encoding='utf-8')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}